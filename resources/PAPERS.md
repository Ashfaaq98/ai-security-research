# PAPERS — AI Security & Red Teaming (Chronological)

Curated list of academic & preprint papers relevant to AI security, LLM red teaming, agentic AI threats, prompt injection, jailbreaks, backdoors, data poisoning and defenses. Sorted newest → oldest (grouped by year). 

---

## 2025


// Security Challenges in AI Agent Deployment: Insights from a Large Scale Public Competition
28 July 2025
https://arxiv.org/pdf/2507.20526


// Prompt Injection 2.0: Hybrid AI Threats
17 July 2025
https://arxiv.org/pdf/2507.13169


// A Systematization of Security Vulnerabilities in Computer Use Agents
7 July 2025
https://arxiv.org/pdf/2507.05445


// The AI Security Zugzwang
https://arxiv.org/pdf/2502.06000



// Red Teaming AI Red Teaming
7 July 2025
https://arxiv.org/pdf/2507.05538


// From Prompt Injections to Protocol Exploits: Threats in LLM-Powered AI Agents Workflows
29 June 2025
https://arxiv.org/pdf/2506.23260

// AIRTBench: Measuring Autonomous AI Red Teaming Capabilities in Language Models
17 June 2025
https://arxiv.org/pdf/2506.14682


// SecurityLingua: Efficient Defense of LLM Jailbreak Attacks via Security-Aware Prompt Compression
15 June 2025
https://arxiv.org/pdf/2506.12707v1


// LLMail-Inject: A Dataset from a Realistic Adaptive Prompt Injection Challenge
11 June 2025
https://arxiv.org/pdf/2506.09956


// A Red Teaming Roadmap Towards System-Level Safety
9 June 2025
https://arxiv.org/pdf/2506.05376


// AgentMisalignment: Measuring the Propensity for Misaligned Behaviour in LLM-Based Agents
4 June 2025
https://arxiv.org/pdf/2506.04018


// JailbreakRadar: Comprehensive Assessment of Jailbreak Attacks Against LLMs
26 May 2025
https://arxiv.org/pdf/2402.05668


// Security Concerns for Large Language Models: A Survey
24 May 2025
https://arxiv.org/pdf/2505.18889v1


//CAIN: Hijacking LLM-Humans Conversations via a Two-Stage Malicious System Prompt Generation and Refining Framework
22 May 2025
https://arxiv.org/pdf/2505.16888


// PANDAGUARD: Systematic Evaluation of LLM Safety in the Era of Jailbreaking Attacks
20 May 2025
https://arxiv.org/pdf/2505.13862v1


// Hidden Ghost Hand: Unveiling Backdoor Vulnerabilities in MLLM-Powered Mobile GUI Agents
20 May 2025
https://arxiv.org/pdf/2505.14418v1


// # Evaluating the Efficacy of LLM Safety Solutions: The Palit Benchmark Dataset
20 May 2025
https://arxiv.org/pdf/2505.13028v2


// Is Your Prompt Safe? Investigating Prompt Injection Attacks Against Open-Source LLMs
20 May 2025
https://arxiv.org/pdf/2505.14368


// Exploring Jailbreak Attacks on LLMs through Intent Concealment and Diversion
20 May 2025
https://www.arxiv.org/pdf/2505.14316


// A Survey of Attacks on Large Language Models
18 May 2025
https://arxiv.org/pdf/2505.12567v1


// LARGO: Latent Adversarial Reflection through Gradient Optimization for Jailbreaking LLMs
16 May 2025
https://arxiv.org/pdf/2505.10838v1



// # Securing RAG: A Risk Assessment and Mitigation Framework
13 May 2025
https://arxiv.org/pdf/2505.08728v1


//  BACKDOORLLM: A Comprehensive Benchmark for Backdoor Attacks and Defenses on Large Language Models
19 May 2025
https://arxiv.org/pdf/2408.12798v2


// Dynamic Attention Analysis for Backdoor Detection in Text-to-Image Diffusion Models
17 May 2025
https://arxiv.org/pdf/2504.20518v2


// ProxyPrompt: Securing System Prompts against Prompt Extraction Attacks
16 May 2025
https://arxiv.org/pdf/2505.11459v1


// Analysing Safety Risks in LLMs Fine-Tuned with Pseudo-Malicious Cyber Security Data
15 May 2025
https://www.arxiv.org/pdf/2505.09974


// Red Teaming the Mind of the Machine: A Systematic Evaluation of Prompt Injection and Jailbreak Vulnerabilities in LLMs
13 May 2025
https://arxiv.org/pdf/2505.04806v2


// Beyond Boundaries: A Comprehensive Survey of Transferable Attacks on AI Systems
12 May 2025
https://arxiv.org/pdf/2311.11796v2


// Jailbreaking the Text-to-Video Generative Models
10 May 2025
https://arxiv.org/pdf/2505.06679


// Engineering Risk-Aware, Security-by-Design Frameworks for Assurance of Large-Scale Autonomous AI Models
9 May 2025
https://arxiv.org/pdf/2505.06409v1


// Jailbreaking and Mitigation of Vulnerabilities in Large Language Models
8 MAY 2025
https://arxiv.org/pdf/2410.15236


// # LlamaFirewall: An open source guardrail system for building secure AI agents
6 May 2025
https://arxiv.org/pdf/2505.03574


// Building Safe GenAI Applications: An End-to-End Overview of Red Teaming for Large Language Models
3 May 2025
https://aclanthology.org/2025.trustnlp-main.23.pdf

// Backdoor Attacks Against Patch-based Mixture of Experts
3 May 2025
https://arxiv.org/pdf/2505.01811


// XBreaking: Explainable Artificial Intelligence for Jailbreaking LLMs
30 April 2025
https://arxiv.org/pdf/2504.21700


// The Automation Advantage in AI Red Teaming
29 April 2025
https://www.arxiv.org/pdf/2504.19855v2

// AegisLLM: Scaling Agentic Systems for Self-Reflective Defense in LLM Security
28 April 2025
https://arxiv.org/pdf/2504.20965v1


// # Securing Agentic AI: A Comprehensive Threat Model and Mitigation Framework for Generative AI Agents
28 Apr 2025
https://arxiv.org/pdf/2504.19956v1

// UNDERSTANDING AND MITIGATING RISKS OF GENERATIVE AI IN FINANCIAL SERVICES
25 April 2025
https://arxiv.org/pdf/2504.20086


// Attention Tracker: Detecting Prompt Injection Attacks in LLMs
23 April 2025
https://arxiv.org/pdf/2411.00348


// A Comprehensive Survey in LLM(-Agent) Full Stack Safety:Data, Training and Deployment
22 April 2025
https://arxiv.org/pdf/2504.15585v1


// # Backdoor Defense in Diffusion Models via Spatial Attention Unlearning
21 Apr 2025
https://arxiv.org/pdf/2504.18563v1


// Security-First AI: Foundations for Robust and Trustworthy Systems
17 April 2025
https://arxiv.org/pdf/2504.16110v1

// # Bypassing Prompt Injection and Jailbreak Detection in LLM Guardrails
16 April 2025
https://arxiv.org/pdf/2504.11168


// Token-Level Constraint Boundary Search for Jailbreaking Text-to-Image Models
15 April 2025
https://arxiv.org/pdf/2504.11106


// Achilles Heel of Distributed Multi-Agent Systems
10 April 2025
https://arxiv.org/pdf/2504.07461v1


// # Bypassing Safety Guardrails in LLMs Using Humor
9 April 2025
https://arxiv.org/pdf/2504.06577v1


// CyberLLMInstruct: A New Dataset for Analysing Safety of Fine-Tuned LLMs Using Cyber Security Data
5 April 2025
https://arxiv.org/pdf/2503.09334v2



// DeBackdoor: A Deductive Framework for Detecting Backdoor Attacks on Deep Models with Limited Data
27 March 2025
https://arxiv.org/pdf/2503.21305


// Defeating Prompt Injections by Design
24 March 2025
https://arxiv.org/pdf/2503.18813?


//AutoRedTeamer: Autonomous Red Teaming with Lifelong Attack Integration
20 March 2025
https://arxiv.org/pdf/2503.15754



//  An Early Categorization of Prompt Injection Attacks on Large Language Models

https://ar5iv.labs.arxiv.org/html/2402.00898

// LeakageDetector: An Open Source Data Leakage Analysis Tool in Machine Learning Pipelines
18 March 2025
https://arxiv.org/pdf/2503.14723


// Revisiting Training-Inference Trigger Intensity in Backdoor Attacks
15 March 2025
https://arxiv.org/pdf/2503.12058


// Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack
26 February 2025
https://arxiv.org/pdf/2404.01833

// Single-pass Detection of Jailbreaking Input in Large Language Models
21 Feb 2025
https://arxiv.org/pdf/2502.15435v1

// Model Supply Chain Poisoning: Backdooring Pre-trained Models via Embedding Indistinguishability
4 Feb 2025
https://arxiv.org/pdf/2401.15883


// Gandalf the Red: Adaptive Security for LLMs
2 Feb 2025
https://arxiv.org/pdf/2501.07927

// How Does a Deep Learning Model Architecture Impact Its Privacy? A Comprehensive Study of Privacy Attacks on CNNs and Transformers
2 Feb 2025
https://arxiv.org/pdf/2210.11049v3

// Distraction is All You Need for Multimodal Large Language Model Jailbreaking
15 February 2025
https://arxiv.org/pdf/2502.10794v1


//# Swiss Cheese Model for AI Safety: A Taxonomy and Reference Architecture for Multi-Layered Guardrails of Foundation Model Based Agents
27 January 2025
https://arxiv.org/pdf/2408.02205v4


// Cross-Modal Transferable Image-to-Video Attack on Video Quality Metrics
14 January 2025
https://arxiv.org/pdf/2501.08415


// Lessons From Red Teaming 100 Generative AI Products
13 Jan 2025
https://arxiv.org/pdf/2501.07238

// SecAlign: Defending Against Prompt Injection with Preference Optimization
13 Jan 2025
https://arxiv.org/pdf/2410.05451v2

// Detecting Malicious AI Agents Through Simulated Interactions
Jan 2025
https://www.arxiv.org/pdf/2504.03726

// Backdoor Token Unlearning: Exposing and Defending Backdoors in Pretrained Language Models
5 January 2025
https://arxiv.org/pdf/2501.03272


---

## 2024


// Automated Progressive Red Teaming
https://arxiv.org/pdf/2407.03876
21 December 2024


// Security of AI Agents
17 December 2024
https://arxiv.org/pdf/2406.08689


//Do Membership Inference Attacks Work on Large Language Models?
16 September 2024
https://arxiv.org/pdf/2402.07841

//POISONBENCH : ASSESSING LARGE LANGUAGE MODEL VULNERABILITY TO DATA POISONING
11 October 2024
https://arxiv.org/pdf/2410.08811v1

// # Automated Red Teaming with GOAT: the Generative Offensive Agent Tester
2 October 2024
https://arxiv.org/pdf/2410.01606


// Insights and Current Gaps in Open-Source LLM Vulnerability Scanners: A Comparative Analysis
16 November 2024
https://arxiv.org/pdf/2410.16527


// Lifting the Veil on the Large Language Model Supply Chain: Composition, Risks, and Mitigations
28 Oct 2024
https://arxiv.org/pdf/2410.21218v1


// Breaking ReAct Agents: Foot-in-the-Door Attack Will Get You In
22 Oct 2024
https://arxiv.org/pdf/2410.16950?

// AI Agents Under Threat: A Survey of Key Security Challenges and Future Pathways
6 Sep 2024
https://arxiv.org/pdf/2406.02630


// Red-Teaming for Generative AI: Silver Bullet or Security Theater?
27 Aug 2024
https://arxiv.org/pdf/2401.15897v3



// The Emerged Security and Privacy of LLM Agent: A Survey with Case Studies
28 July 2024
https://arxiv.org/pdf/2407.19354v1


// RedAgent: Red Teaming Large Language Models with Context-aware Autonomous Language Agent
23 July 2024
https://arxiv.org/pdf/2407.16667


// ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs
7 June 2024
https://arxiv.org/pdf/2402.11753


// # User Inference Attacks on Large Language Models
26 Feb 2024
https://ar5iv.labs.arxiv.org/html/2310.09266


- **OpenAI: External Red Teaming for AI Models and Systems (report)** — Sep 2024
  https://cdn.openai.com/papers/openais-approach-to-external-red-teaming.pdf

- **Systematically Analyzing Prompt Injection Vulnerabilities in Diverse LLM Architectures** - Oct 2024
  https://arxiv.org/pdf/2410.23308



---

## 2023 and earlier 

- **Model Leeching: An Extraction Attack Targeting LLMs** — 19 Sep 2023  
  https://arxiv.org/pdf/2309.10544

- **More Than Privacy: Applying Differential Privacy in Key Areas of AI** — 5 Aug 2020  
  https://arxiv.org/pdf/2008.01916


---


