# PAPERS — AI Security & Red Teaming (Chronological)

Curated list of academic & preprint papers relevant to AI security, LLM red teaming, agentic AI threats, prompt injection, jailbreaks, backdoors, data poisoning and defenses. Sorted newest → oldest (grouped by year). Add minimal summary if you later want.

---

## 2025
- **Security Challenges in AI Agent Deployment: Insights from a Large Scale Public Competition** — 28 Jul 2025  
  https://arxiv.org/pdf/2507.20526

- **Prompt Injection 2.0: Hybrid AI Threats** — 17 Jul 2025  
  https://arxiv.org/pdf/2507.13169

- **A Systematization of Security Vulnerabilities in Computer Use Agents** — 7 Jul 2025  
  https://arxiv.org/pdf/2507.05445

- **Red Teaming AI Red Teaming** — 7 Jul 2025  
  https://arxiv.org/pdf/2507.05538

- **From Prompt Injections to Protocol Exploits: Threats in LLM-Powered AI Agents Workflows** — 29 Jun 2025  
  https://arxiv.org/pdf/2506.23260

- **AIRTBench: Measuring Autonomous AI Red Teaming Capabilities in Language Models** — 17 Jun 2025  
  https://arxiv.org/pdf/2506.14682

- **SecurityLingua: Efficient Defense of LLM Jailbreak Attacks via Security-Aware Prompt Compression** — 15 Jun 2025  
  https://arxiv.org/pdf/2506.12707v1

- **LLMail-Inject: A Dataset from a Realistic Adaptive Prompt Injection Challenge** — 11 Jun 2025  
  https://arxiv.org/pdf/2506.09956

- **A Red Teaming Roadmap Towards System-Level Safety** — 9 Jun 2025  
  https://arxiv.org/pdf/2506.05376

- **AgentMisalignment: Measuring the Propensity for Misaligned Behaviour in LLM-Based Agents** — 4 Jun 2025  
  https://arxiv.org/pdf/2506.04018

---

## 2024 (selected)
- **Security of AI Agents** — 17 Dec 2024  
  https://arxiv.org/pdf/2406.08689

- **POISONBENCH: Assessing Large Language Model Vulnerability to Data Poisoning** — 11 Oct 2024  
  https://arxiv.org/pdf/2410.08811v1

- **JailbreakRadar: Comprehensive Assessment of Jailbreak Attacks Against LLMs** — 26 May 2025 (but overlaps with 2024 research timelines)  
  https://arxiv.org/pdf/2402.05668

- **Red-Teaming for Generative AI: Silver Bullet or Security Theater?** — 27 Aug 2024  
  https://arxiv.org/pdf/2401.15897v3

- **The Emerged Security and Privacy of LLM Agent: A Survey with Case Studies** — 28 Jul 2024  
  https://arxiv.org/pdf/2407.19354v1

- **ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs** — 7 Jun 2024  
  https://arxiv.org/pdf/2402.11753

---

## 2023 and earlier (not exhaustive)
- **Model Leeching: An Extraction Attack Targeting LLMs** — 19 Sep 2023  
  https://arxiv.org/pdf/2309.10544

- **More Than Privacy: Applying Differential Privacy in Key Areas of AI** — 5 Aug 2020  
  https://arxiv.org/pdf/2008.01916

- **OpenAI: External Red Teaming for AI Models and Systems (report)** — (OpenAI)  
  https://cdn.openai.com/papers/openais-approach-to-external-red-teaming.pdf

---

## Notes & curation suggestions
- Add short 1–2 line summaries under each entry when you (or contributors) have time — helps quick triage.
- Consider storing canonical PDFs in a `papers/` folder (if licensing/permissions allow) and add `papers/<slug>.md` with a one-paragraph summary and key findings.
- Use tags by paper (e.g., `prompt-injection`, `data-poisoning`, `backdoor`, `agent-misalignment`) for filtering in the future.

